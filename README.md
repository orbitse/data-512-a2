# data-512-a2: Bias in Wikipedia Article Data

## Purpose of this Project  

The goal of this project is to explore the concept of 'bias' through data by analyzing Wikipedia articles on political figures from a variety of countries.  

The dataset we will analyze includes information on political articles on English language Wikipedia, the populations for each country as of 2015, and the predicted quality scores for the political articles from a Wikipedia machine learning service.  

The project will quantify the number of Wikipedia articles (also called pages) devoted to politicians within each country, the quality of those political articles, and consider how those measurements vary between countries.  

The analysis will include a series of visualizations that show:  
 1. The countries with the greatest and least coverage of politicians on Wikipedia on a per capita basis.  
 2. The countries with the highest and lowest proportion of high quality articles about politicians. 

This README file and the Jupyter Notebook file, `hcds-a2-data-curation.ipynb`, contain the information and
references needed to reproduce the analysis, including a description of the data and all relevant resources
and documentation, with hyperlinks to those resources.  

## Data Used in Project

The data used in this project was generated by combining three datasets from different sources:  
 - the Wikipedia article dataset,
 - the population dataset, and
 - the article quality prediction dataset.

### Wikipedia Article Data

This English-language Wikipedia article data about politicians came from the "Category:Politicians by nationality" and corresponding subcategories. The Wikipedia article dataset can be found on [Figshare](https://figshare.com/articles/Untitled_Item/5513449). The article data was extracted using the Wikimedia API and saved as a CSV file named `page_data.csv`. A copy of that dataset is also available in this repository: [page_data.csv](https://raw.githubusercontent.com/orbitse/data-512-a2/master/page_data.csv).
  
The columns in the `page_data.csv` file are:  
 1. country - the country name, extracted from the category name  
 2. page - the Wikipedia page title  
 3. last_edit - the edit ID of the last edit to the page, also called the revision_id 
 
 #### Wikipedia Data License and Terms of Use
 
The Wikipedia data and the code used to generate that data are released under the CC-BY-SA 4.0 license.  

This Wikimedia Foundation data is licensed under an Apache 2.0 License, which includes in part:  

> Unless required by applicable law or agreed to in writing, software  
> distributed under the License is distributed on an "AS IS" BASIS,  
> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  
> See the License for the specific language governing permissions and  
> limitations under the License.  

For more information on the this license, see [Apache 2.0 License](http://www.apache.org/licenses/LICENSE-2.0).

__Terms of Use__  

The use of Wikipedia data is subject to the Wikimedia Foundation terms of use, a summary along with the complete terms are available [here](https://wikimediafoundation.org/wiki/Terms_of_Use/en).   
 
### Population Data 

The population data for 210 countries comes from the Population Research Bureau's (PRB) 2015 World Population Data Sheet, available on this [PRB Data website](http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=14). If you would like to download the population data, follow the link and click the Excel icon on the right side, as shown in the screenshot below.  
  
![Population Data](https://raw.githubusercontent.com/orbitse/data-512-a2/master/population_data.jpeg)  
  
However, that same population dataset is also available in this repository, [Population Mid-2015.csv](https://raw.githubusercontent.com/orbitse/data-512-a2/master/Population%20Mid-2015.csv).  

The columns in the Population Mid-2015.csv file are:
  1. Location	- the country name   
  2. Location Type	- Country   
  3. TimeFrame	- Mid-2015  
  4. Data Type	- Number  
  5. Data - the population value encoded as a number with commas  

__Population Data License__  

It's not clear if the population data is subject to a license.  

However, the stated mission of the PRB is to provide access to data. "The Population Reference Bureau informs people around the world about population, health, and the environment, and empowers them to use that information to advance the well-being of current and future generations." [PRB's About Page](http://www.prb.org/About.aspx) (as of Oct. 27, 2017) Additionally, the PRB's annual report states that it is "committed to making all of its products and resources publicly available by disseminating them widely through both print and digital channels, and in innovative formats." [See report](http://www.prb.org/About/Annual-Report.aspx)  

### Article Quality Prediction Data 

The predicted quality scores for each article in the Wikipedia dataset comes from a Wikimedia API endpoint for a machine learning system called __ORES__ ("Objective Revision Evaluation Service"). ORES estimates the quality of an article at a particular point in time, and assigns a series of probabilities that the article is best described by one of the categories listed below.  

The range of quality scores are, from best to worst:  
  1.	FA - Featured article
  2.	GA - Good article
  3.	B - B-class article
  4.	C - C-class article
  5.	Start - Start-class article
  6.	Stub - Stub-class article  

These quality scores are a sub-set of quality assessment categories developed by Wikipedia editors. For more information about the scores, see [Project Assessment](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_assessment#Grades).  

The ORES API documentation can be found [here](https://www.mediawiki.org/wiki/ORES) and the web API is [here](https://ores.wikimedia.org/v3/). The API requires a revision ID, which is the third column in the Wikipedia dataset (originally titled "last_edit"), and the machine learning model, which is "wp10". 

When you query the API, the ORES returns a JSON object that includes a predicted quality score, as well as the probability values for each of the six possible quality scores. But, for the analysis in this project, you only need the predicted quality score value, not the probabilities.  

This is an example of a response in the JSON format from the ORES API:  
>   {"enwiki": {    
 >>      "models": {   
 >>>             "wp10": {  
 >>>>                   "version": "0.5.0"  
 >>>                       }  
 >>              },   
 >>       "scores": {    
 >>>                 "774499188": {   
 >>>>                           "wp10": {    
 >>>>>                               "score": {    
 >>>>>                                   "prediction": "Stub",   
 >>>>>>                                            "probability": {      
  >>>>>>>                                                "B": 0.03488477079112925,   
  >>>>>>>>                                               "C": 0.06953258948284814,  
  >>>>>>>>                                               "FA": 0.0025762575670963965,  
  >>>>>>>>                                               "GA": 0.007911851615317388,   
  >>>>>>>>                                               "Start": 0.4106575723489943,   
  >>>>>>>>                                               "Stub": 0.4744369581946146   
 >>>>>>>                                     }  
 >>>>>>                               }  
 >>>>>                        }    
 >>>>                 }    
 >>>           }   
 >>      }    
>  }  

__Article Quality Prediction Data License__  

The Wikipedia data, and the code used to generate that data, are released under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](https://creativecommons.org/licenses/by-sa/3.0/).

## Tools Used in Project

This project uses the open-source web application Jupyter Notebook. To download Jupyter Notebook, see [Installation](https://jupyter.org/install.html) and for more information, see [Documentation](https://jupyter.org/documentation.html). 

The code in the Jupyter Notebook project file, `hcds-a2-data-curation.ipynb`, is written in Python 3. You also need to have Python installed in order to run the Jupyter Notebook application. To download a version of Python 3, like Python 3.6, see [Download](https://www.python.org/downloads/) and [Beginner's Guide](https://wiki.python.org/moin/BeginnersGuide).  

Alternatively, you can download a version of Python 3 by downloading Anaconda ([Download](https://www.anaconda.com/download/), [Documentation](https://docs.anaconda.com/anaconda/)).

In addition to Python 3.X, the following Python libraries are needed to run the code in `hcds-a2-data-curation.ipynb`.  
  - [Matplotlib](https://matplotlib.org) : will be used for data visualization
  - [Pandas](http://pandas.pydata.org) : will be used for data processing
  - [Seaborn](http://seaborn.pydata.org) -- Note: this library is only for style purposes and is not essential
  
  If necessary, you can pip install any of the above libraries, for example:  
  `pip install matplotlib` or `pip3 install matplotlib`.  
  
  Additionally, if you have any version of Python 3.X installed, you should already have the csv, json,  
  and requests libraries installed. These dependency libraries will be used to get and save data in the notebook  
  file, `hcds-a2-data-curation.ipynb`. 

## Data Visualization  

This visualization of the combined datasets was created with matplotlib.  

![Wikipedia Data Plot](https://raw.githubusercontent.com/orbitse/data-512-a2/master/WikipediaBiasDataPlot.png)  

 See the Jupyter Notebook file, `hcds-a2-data-curation.ipynb`, in this repository for the code to create this visualization. 

## Writeup 

Write a few paragraphs reflecting on what you have learned, what you found, what (if anything) surprised you about your findings, and/or what theories you have about why any biases might exist (if you find they exist). You can also include any questions this assignment raised for you about bias, Wikipedia, or machine learning. 
